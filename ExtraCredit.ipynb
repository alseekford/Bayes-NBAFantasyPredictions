{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c50df8-0df6-47e4-a1df-ba338dbd34c3",
   "metadata": {},
   "source": [
    "# Extra Credit Assignment\n",
    "\n",
    "### Noah Dunn\n",
    "\n",
    "### 12/07/2022\n",
    "\n",
    "On my honor as a student at the University of Virginia, I have neither given nor received unathorized aid on this assignment  -  *Noah Dunn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc163cb-dbd4-49fc-9cf0-0ff935f3cd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahdunn/opt/anaconda3/lib/python3.9/site-packages/theano/configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "WARNING (aesara.configdefaults): g++ not detected!  Aesara will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set Aesara flags cxx to an empty string.\n",
      "WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import theano\n",
    "import pymc as pm\n",
    "import sklearn\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#import wikipedia\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.stats as st\n",
    "import pymc as pm\n",
    "import scipy.linalg as la\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "sns.set_style('white')\n",
    "os.chdir('/Users/noahdunn/Desktop/MSDS/DS6040/Homeworks/ExtraCredit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ecf11-6b92-4a56-bb15-4d3f68c0835a",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "With the BART Example code use BART to predict SAT scores using\n",
    "the percent takes and school spending as predictors. Plot the 95% HDI for\n",
    "posterior SAT scores vs. percent takers and comment on the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262a1a8f-bbe8-4ceb-a5d1-15f303c86a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC v4.2.2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   State      50 non-null     object \n",
      " 1   Spend      50 non-null     float64\n",
      " 2   StuTeaRat  50 non-null     float64\n",
      " 3   Salary     50 non-null     float64\n",
      " 4   PrcntTake  50 non-null     int64  \n",
      " 5   SATV       50 non-null     int64  \n",
      " 6   SATM       50 non-null     int64  \n",
      " 7   SATT       50 non-null     int64  \n",
      "dtypes: float64(3), int64(4), object(1)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pymc_bart as pmb\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "print(f\"Running on PyMC v{pm.__version__}\")\n",
    "\n",
    "RANDOM_SEED = 1934\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv('/Users/noahdunn/Desktop/MSDS/DS6040/Homeworks/ExtraCredit/Guber1999data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e1999e-bdd2-498f-92c2-54ee4e7a2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Spend', 'PrcntTake']]\n",
    "Y = df['SATT']\n",
    "\n",
    "meanx = X.mean().values\n",
    "scalex = X.std().values\n",
    "zX = ((X-meanx)/scalex).values\n",
    "\n",
    "meanY = Y.mean()\n",
    "scaleY = Y.std()\n",
    "zY = ((Y-meanY)/scaleY).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bda4d9-0b0b-496e-8b71-5b76fea338c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">NUTS: [σ]\n",
      ">PGBART: [μ]\n",
      "WARNING (aesara.configdefaults): g++ not detected!  Aesara will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set Aesara flags cxx to an empty string.\n",
      "WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "WARNING (aesara.configdefaults): g++ not detected!  Aesara will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set Aesara flags cxx to an empty string.\n",
      "WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "WARNING (aesara.configdefaults): g++ not detected!  Aesara will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set Aesara flags cxx to an empty string.\n",
      "WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "WARNING (aesara.configdefaults): g++ not detected!  Aesara will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set Aesara flags cxx to an empty string.\n",
      "WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2404' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      30.05% [2404/8000 01:29&lt;03:28 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pm.Model() as model_bikes:\n",
    "    σ = pm.HalfNormal(\"σ\", sigma = zY.std())\n",
    "    μ = pmb.BART(\"μ\", zX, zY, m=50)\n",
    "    y = pm.Normal(\"y\", μ, σ, observed=zY)\n",
    "    idata_bikes = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96c7c3-c834-4f50-9164-b9bf696b0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT vs. Takers\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sat = idata_bikes.posterior[\"μ\"]\n",
    "az.plot_hdi(X['PrcntTake'], sat, smooth=True)\n",
    "az.plot_hdi(X['PrcntTake'], sat, hdi_prob=0.5, smooth=True, plot_kwargs={\"alpha\": 0})\n",
    "az.plot_hdi(X['PrcntTake'], sat, hdi_prob=0.1, smooth=True, plot_kwargs={\"alpha\": 0})\n",
    "ax.set_xlabel(\"% of Students Taking SAT\")\n",
    "ax.set_ylabel(\"SAT Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ec75e-f80c-4b3f-bec2-c91ce909d1d3",
   "metadata": {},
   "source": [
    "#### Comments on relationship between SAT scores and % takers:  As the percent of students that take the SAT increases, the average score (generally) decreases. This is likely due to latent factors, such as the idea that in areas where there is a relatively small number of students taking the SAT, they are likely the only ones with a 'genuine chance' to go to a university where a good score might be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc14530-239f-4299-8342-aa8ccd746604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e700afba-e228-4a0c-9223-96c1ff0dcef7",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Explain the difference between traditional autoencoders and variational autoencoders. Give an example of how variational autoencoders\n",
    "are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78031be1-9cb5-44b0-82e4-034ce9fd225c",
   "metadata": {},
   "source": [
    "#### Difference:  A traditional autoencoder takes input data (image, vector, etc.) with high dimensionality and runs it through a neural network to compress the data into a smaller representation. The data then goes through a bottleneck where it is represented by the smallest number of variables, and then it is 'decoded' through another neural network that attempts to reconstruct the original data input. A variational autoencoder uses a similar neural network 'encoder' to compress data and a similar neural network 'decoder' to reconstruct the data. However, instead of mapping the input to a fixed vector (as would happen with a traditional encoder), it is mapped to a distribution. This means that two bottleneck vectors are used instead of one, with the first representing the mean of the distribution and the second representing the standard deviation of the distribution. This means that whenever you need to feed a vector through the decoder, the only thing that is needed is a sample from the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b6a59-cdc2-4a64-a375-a53267c5b6c8",
   "metadata": {},
   "source": [
    "#### Variational Autoencoder Example:  One example is in reinforcement learning. The problem here is that there are very sparse rewards requiring a long time in order to train models. So, variational autoencoders are used as a sort of feature extractor, and then the goal is to be able to run another program/model/etc. on the compressed representation of the data produced by the variational autoencoder instead of on the entire (large) input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae2475-647d-4a3f-b4a7-16a4e4050502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6adce1-5530-409e-b985-7a4ffeb3d758",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Neural networks are a graphical model, Markov Networks, that can be\n",
    "analyzed with Bayesian methods and Boltzmann machines are examples.\n",
    "Why is training or learning computationally challenging for Boltzmann\n",
    "machines? Explain how restricted Boltzmann machines (RBM) are easier\n",
    "to train. What approaches do we use for training RBM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7327c3d-dec7-48d7-aab2-dd026df8191d",
   "metadata": {},
   "source": [
    "#### Why is training computationally challenging for Boltzmann machines:  In most real world problems, the number of cliques within the data being looked at is very large (and even if they are only of modest sizes). Because Boltzmann machines are made of nodes/neurons, and the connection matrix W describes whether neurons are connected with one another (can get very large), this makes the computation of $Z$ (the normalizing term) and $p(x_v, x_h)$ computationally intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a152b-b1a7-4ed4-8d51-ba822baa293b",
   "metadata": {},
   "source": [
    "#### Why are restricted Boltzmann machines easier to train:  RBMs have a single hidden layer and a single visible layer. While the computation of $Z$ is intractable, the two layer structure of an RBM makes it feasible to calculate conditional distributions $P(x_h | x_v)$ and $P(x_v | x_h)$. The fact that there is no intra-layer communication between nodes (nodes within each of the two layers do not communicate with other nodes in that layer) allows for quicker training algorithms, such as contrastive divergence, to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad733dab-215a-4381-b746-5a796a9ba0b6",
   "metadata": {},
   "source": [
    "#### What approaches do we use to train RBMs:  RBMs are often trained using contrastive divergence, which uses Gibbs sampling inside a gradient descent procedure that computes updates for weights. This approximates the relationship between the error in the network and its weights, and then represents this as a slope/gradient in order to perform gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3a2f5-9618-4fd6-979b-966dc54ec67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7492bb18-d05c-4b44-8ce8-14cda8ee4097",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "The following questions are based on reading and running the jupyter\n",
    "notebook, pymc3-variation-inference-neural-network.ipynb,\n",
    "by Thomas Wiecki, updated by Maxim Kochurov as provided in their blog\n",
    "post. Run the notebook and then answer these questions. Notice that this\n",
    "notebook is in pymc3 not pymc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a041173-449f-4d6e-8e5e-0a421511d28b",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Wieki says that an advantage to using Bayesian modeling with neural\n",
    "networks and deep learning is that “we could train the model specifically on samples it is most uncertain about.” Explain how he finds\n",
    "these samples in this example. Explain how you would implement\n",
    "his suggestion (you do not have to actually implement this)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026d786-8905-4151-b178-ec0325a43ae1",
   "metadata": {},
   "source": [
    "#### How he finds the samples with the most uncertainty in this example:  By training the model primarily on samples from the high-uncertainty region found when the standard deviation of the posterior predictive is graphed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f040573-9e8e-4837-b941-abb61abca1c2",
   "metadata": {},
   "source": [
    "#### How could I implement his suggestion:  One way to do this would be through the use of Support Vector Machines that focus on values close to the decision boundary between the two 'clusters' in the model. SVMs only use support vectors that are close to the hyperplane being used to divide the data points, so this approach would make sense in order to implement the above solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e1e8b-5992-4581-bf8b-97b552548bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd8ee177-7632-4097-a29c-8ca43bc8fa8c",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Wieki also says that another advantage to Bayesian modeling with\n",
    "neural network and deep learning is that “We also get uncertainty\n",
    "estimates of our weights which could inform us about the stability\n",
    "of the learned representations of the network.” Discuss what the\n",
    "uncertainty estimates for the weights found for the example in this\n",
    "notebook imply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff7812-5e00-4c52-b6ff-bfc1c1f29fd2",
   "metadata": {},
   "source": [
    "#### What the uncertainty estimates for the weights found for the example in this notebook imply:  In general, these uncertainty estimates tell us about how confident we are in the decision boundary being produced by the neural network. For example, if there is a high uncertainty/less stability for an area, it implies that bias could be present - if small changes to data were made around this area, the outcome could be very different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b7a7d-e215-477a-80df-c0fab493715d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52d39f6a-3115-4739-ae38-f2073b289289",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Explain how the Gaussian priors help to regularize the weights in the\n",
    "neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab636f0-2448-4390-bbad-bb53f30da20f",
   "metadata": {},
   "source": [
    "#### How Gaussian priors help regularize weights in the neural network:  In neural networks, if weights are large then this often means that the model is overfitting the training data that it was fed. In order to mitigate this issue, L2-regularization can be helpful for bringing the weights closer to 0. A Gaussian prior centered on 0 will act similarly, by producing a higher prior probability of the weights being close to 0, with tails of lower probability going out into areas where the weights might be very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14c6f6-3fe7-4c94-ac8e-b6ce36c35777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ea85846-cc77-4697-8182-cf9d1944a693",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Why do we use a variational approximation instead of sampling for\n",
    "estimating the posterior of the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc6132-e132-4010-ba06-00f2c0cdfdea",
   "metadata": {},
   "source": [
    "#### Why use variational approximation instead of sampling for estimating the posterior of the weights:  We use variational approximation instead of sampling because it can scale better and will run faster, especially as neural networks get deeper and deeper with more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90844d47-3ac0-42cb-a5c2-6d32544cf4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e965c29-527d-41db-905a-47ee77ddaa74",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "Change the prior distributions for all three sets of the neural net\n",
    "weights to Cauchy with location (alpha) = 0 and scale (beta) =\n",
    "2. Rerun the remaining cells in the notebook and comment on any\n",
    "changes you see from this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f035d8e-6b58-4743-9b1b-0fadc012914f",
   "metadata": {},
   "source": [
    "#### What changes come from changing the prior distributions of neural net weights:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec21a4-6b22-4b97-a965-cbbdb23e7c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
